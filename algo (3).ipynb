{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f5bcf7",
   "metadata": {},
   "source": [
    "# **Algorithm Group 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da3c30c",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1969665",
   "metadata": {},
   "source": [
    "### * Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee9cc00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import ssl\n",
    "import matplotlib.pyplot as plt # for plotting graph\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc002973",
   "metadata": {},
   "source": [
    "### * Base Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b8356f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStopWords():\n",
    "    f = open(\"C:\\\\Users\\\\User\\\\PycharmProjects\\\\Algo\\\\Files\\\\stop_words.txt\", 'r') # read text file storing stopwords\n",
    "    stop_words = []\n",
    "    for word in f: # return list of stopwords\n",
    "        stop_words.append(word[:-1])\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f64645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripTags(pageContents): # web scrapping\n",
    "    pageContents = str(pageContents) # convert to string\n",
    "    startLoc = pageContents.find(\"<p>\")\n",
    "    endLoc = pageContents.rfind(\"<br/>\")\n",
    "\n",
    "    pageContents = pageContents[startLoc:endLoc]\n",
    "\n",
    "    inside = 0\n",
    "    text = ''\n",
    "\n",
    "    for char in pageContents:\n",
    "        if char == '<':\n",
    "            inside = 1\n",
    "        elif (inside == 1 and char == '>'):\n",
    "            inside = 0\n",
    "        elif inside == 1:\n",
    "            continue\n",
    "        else:\n",
    "            text += char\n",
    "\n",
    "    return text\n",
    "\n",
    "# Given a text string, remove all non-alphanumeric\n",
    "# characters (using Unicode definition of alphanumeric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b1eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripNonAlphaNum(text):\n",
    "    import re\n",
    "    return re.compile(r'\\W+', re.UNICODE).split(text)\n",
    "\n",
    "# Given a list of words, return a dictionary of\n",
    "# word-frequency pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e1a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordListToFreqDict(wordlist):\n",
    "    wordfreq = [wordlist.count(p) for p in wordlist]\n",
    "    return dict(list(zip(wordlist,wordfreq)))\n",
    "\n",
    "# Sort a dictionary of word-frequency pairs in\n",
    "# order of descending frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5b45615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortFreqDict(freqdict):\n",
    "    return {k: v for k, v in sorted(freqdict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "# Given a list of words, remove any that are\n",
    "# in a list of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc11733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(wordlist, stopwords): # remove stopwords in the word list\n",
    "    return [w for w in wordlist if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eab9c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positiveWords():\n",
    "    f = open(\"C:\\\\Users\\\\User\\\\PycharmProjects\\\\Algo\\\\Files\\\\positive_words.txt\", 'r') # read text file storing positive words\n",
    "    content_list = f.read().split(\",\")\n",
    "\n",
    "    positive_words = []\n",
    "    for word in content_list: # return list of positive words\n",
    "        positive_words.append(word.strip())\n",
    "    return positive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "118f95b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negativeWords():\n",
    "    f = open(\"C:\\\\Users\\\\User\\\\PycharmProjects\\\\Algo\\\\Files\\\\negative_words.txt\", 'r') # read text file storing negative words\n",
    "    content_list = f.read().split(\",\")\n",
    "    negative_words = []\n",
    "    for word in content_list: # return list of negative words\n",
    "        negative_words.append(word.strip())\n",
    "    return negative_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c157a24",
   "metadata": {},
   "source": [
    "### * Rabin Karp Algortihm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4174fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rabinkarp(T,P,d,q):  # define function for Rabin Karp Algorithm\n",
    "    n = len(T)  # length of text\n",
    "    m = len(P)  # length of pattern\n",
    "    h = d ** (m - 1) % q\n",
    "    p = 0  # hash value of pattern\n",
    "    t = 0  # hash value of` text\n",
    "    # assign indexes\n",
    "    i = 0\n",
    "    s = 0\n",
    "    count = 0;\n",
    "\n",
    "    # for loop to calculate hash value\n",
    "    for i in range(m):\n",
    "        p = (d * p + ord(P[i])) % q  # calculate hash value of pattern\n",
    "        t = (d * t + ord(T[i])) % q  # calculate hash value of first window of text only\n",
    "\n",
    "    # rolling hash to slide to the next window\n",
    "    for s in range(n - m + 1):\n",
    "        if p == t:  # check if current hash value of text window is same as pattern\n",
    "            for i in range(m):  # check for characters of current window of text one by one\n",
    "                if T[i + s] != P[i]:  # if character of text and pattern is not same, break\n",
    "                    break\n",
    "                else:\n",
    "                    i += 1  # if character of text and pattern same, increment 1 to check next character\n",
    "\n",
    "            if i == m:  # index is same as length of pattern\n",
    "                count += 1\n",
    "\n",
    "        if s < (n - m):  #\n",
    "            t = (d * (t - ord(T[s]) * h) + ord(T[s + m])) % q  # calculate the hash value of next text window\n",
    "          # subtract with hash value of previous character and add hash value of next character\n",
    "\n",
    "            if t < 0:  # in case the value of t is negative, make it positive\n",
    "                t = t + q\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cf41e1",
   "metadata": {},
   "source": [
    "### * KMP Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fed6de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMPSearch(pat, txt):\n",
    "    count = 0\n",
    "    M = len(pat)  # length of pattern\n",
    "    N = len(txt)  # length of String\n",
    "    j = 0  # index for pat[]\n",
    "    i = 0  # index for txt[]\n",
    "\n",
    "    # create an array pi[] that will hold the longest matching prefix suffix values for the pattern\n",
    "    pi = [0] * M  # size of array is the length of the pattern\n",
    "\n",
    "    # Preprocess the pattern (calculate pi[] array)\n",
    "    computePIArray(pat, M, pi)\n",
    "    # while i has not reached end of the String\n",
    "\n",
    "    while i < N:  # —-------------------- O(n)\n",
    "      # character in String matches with pattern\n",
    "      if pat[j] == txt[i]:\n",
    "        i += 1  # increment index of txt[]\n",
    "        j += 1  # increment index of pat[]\n",
    "\n",
    "      # all characters in pattern match in the String\n",
    "      if j == M:\n",
    "        count += 1\n",
    "        j = pi[j - 1]  # continue searching for pattern in the rest of the String\n",
    "\n",
    "      # mismatch after j matches\n",
    "      elif i < N and pat[j] != txt[i]:\n",
    "        if j > 0:\n",
    "            j = pi[j - 1]\n",
    "        else:\n",
    "            i += 1  # continue searching until find a match with j[0]\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad34dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePIArray(pat, M, pi):\n",
    "    k = 0  # length of the previous longest prefix suffix\n",
    "\n",
    "    pi[0] = 0  # pi[0] is always 0\n",
    "    q = 1\n",
    "\n",
    "    # the loop calculates pi[i] for i = 1 to M-1\n",
    "    while q < M:  # —-------------------- O(m)\n",
    "        if pat[q] == pat[k]:\n",
    "            k += 1  # increment k by 1\n",
    "            pi[q] = k\n",
    "            q += 1  # increment q by 1\n",
    "        else:\n",
    "            if k != 0:\n",
    "                k = pi[k - 1]  # value of k is the value of pi of index before k\n",
    "                # compare matches with next value\n",
    "                # if does not match, compare next until k = 0\n",
    "\n",
    "            else:\n",
    "                pi[q] = 0  # no matches with previous pat[] pi[q]= 0\n",
    "                q += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1d71f",
   "metadata": {},
   "source": [
    "### * NLTK Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc6ae1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.downloader.download('vader_lexicon') # donwload package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa372b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # import sentiment intensity analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bae7937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(url):\n",
    "    pos_word_list = [] # create list for positive words \n",
    "    count_pos_word = [] # create list for counts of positive words\n",
    "    neg_word_list = [] # create list for negative words \n",
    "    count_neg_word = [] # create list for counts of negative words\n",
    "    neu_word_list = [] # create list for neutral words \n",
    "    count_neu_word = []  # create list for counts of neutral words\n",
    "\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    response = urllib.request.urlopen(url) # read article from the link\n",
    "    html = response.read().decode('UTF-8') # deocde the web content\n",
    "    text = stripTags(html).lower() # remove all non-alphanumeric words\n",
    "    fullwordlist = stripNonAlphaNum(text) # get word list\n",
    "    wordlist = removeStopwords(fullwordlist, getStopWords()) # remove stopwords\n",
    "    dictionary = wordListToFreqDict(wordlist) # get the frequency of each word\n",
    "\n",
    "    # alternative algo method\n",
    "    for s in dictionary:\n",
    "        if (sid.polarity_scores(str(s))['compound']) >= 0.5:\n",
    "            pos_word_list.append(str(s)) # positive words\n",
    "            count_pos_word.append(dictionary[s])\n",
    "        elif (sid.polarity_scores(str(s))['compound']) <= -0.5:\n",
    "            neg_word_list.append(str(s)) # negative words\n",
    "            count_neg_word.append(dictionary[s])\n",
    "        else:\n",
    "            neu_word_list.append(str(s)) # neutral words\n",
    "            count_neu_word.append(dictionary[s])\n",
    "            \n",
    "    totalPositiveWords = 0\n",
    "    for i in count_pos_word:\n",
    "        totalPositiveWords += i # total number of positive words\n",
    "\n",
    "    totalNegativeWords = 0\n",
    "    for i in count_neg_word:\n",
    "        totalNegativeWords += i # total number of negative words\n",
    "\n",
    "    totalNeutralWords = 0\n",
    "    for i in count_neu_word:\n",
    "        totalNeutralWords += i # total number of neutral words\n",
    "\n",
    "    print(\"Total Positive Words : \" + str(totalPositiveWords))\n",
    "    print(\"Total Negative Words : \" + str(totalNegativeWords))\n",
    "\n",
    "    result = [totalPositiveWords, totalNegativeWords, totalNeutralWords] # return number of all words\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69a106",
   "metadata": {},
   "source": [
    "### * Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "252e0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "country =[]\n",
    "japan = [] # Articles link for country Japan\n",
    "japan.append('https://english.kyodonews.net/news/2022/05/3645980f7e25-focus-japan-economy-may-rebound-but-faces-headwind-amid-price-surge.html')\n",
    "japan.append('https://www.hurriyetdailynews.com/japans-economy-shrinks-in-173873')\n",
    "japan.append('https://www.nippon.com/en/in-depth/d00778/')\n",
    "japan.append('https://japantoday.com/category/business/update2-japan-economic-report-drops-mention-of-coronavirus')\n",
    "japan.append('https://www.brinknews.com/the-japanese-economy-has-reinvented-itself/')\n",
    "country.append(japan)\n",
    "\n",
    "china = [] # Articles link for country China\n",
    "china.append('https://www.worldbank.org/en/country/china/overview#1')\n",
    "china.append('https://www.frontiersin.org/articles/10.3389/fpubh.2021.787190/full')\n",
    "china.append('https://observer.com/2022/05/china-economy-slowdown-zero-covid-lockdown/')\n",
    "china.append('https://www.straitstimes.com/asia/east-asia/how-covid-19-restrictions-are-choking-chinas-economy-asian-insider')\n",
    "china.append('https://www.aljazeera.com/economy/2022/5/11/china-resorts-to-old-economic-tools-as-lockdowns-bite')\n",
    "country.append(china)\n",
    "\n",
    "UAE = [] # Articles link for country UAE\n",
    "UAE.append('https://www.arabnews.com/node/2092066/business-economy')\n",
    "UAE.append('https://www.khaleejtimes.com/property/dubai-apartment-rents-increase-by-up-to-25-in-some-areas')\n",
    "UAE.append('https://www.arabnews.com/node/2092251/business-economy')\n",
    "UAE.append('https://gulfnews.com/uae/government/shorter-uae-work-week-highlights-high-adaptability-of-government-world-economic-forum-hears-1.88177737')\n",
    "UAE.append('https://www.khaleejtimes.com/business/uae-highlights-national-vision-at-world-economic-forum')\n",
    "country.append(UAE)\n",
    "\n",
    "canada = [] # Articles link for country Canada\n",
    "canada.append('https://www.investopedia.com/articles/investing/042315/fundamentals-how-canada-makes-its-money.asp')\n",
    "canada.append('https://www3.ohrc.on.ca/en/search/site')\n",
    "canada.append('https://www.canada.ca/en/impact-assessment-agency/services/policy-guidance/practitioners-guide-impact-assessment-act/analyzing-health-social-economic-effects-impact-assessment-act.html')\n",
    "canada.append('https://www.canada.ca/en/department-finance/services/publications/economic-fiscal-snapshot/overview-economic-response-plan.html')\n",
    "canada.append('https://theconversation.com/antiquated-thinking-about-old-age-hinders-canadas-economic-and-social-development-182367')\n",
    "country.append(canada)\n",
    "\n",
    "greatBritain = [] # Articles link for country Great Britain\n",
    "greatBritain.append('https://www.gov.uk/government/publications/uk-and-indiana-trade-and-economic-memorandum-of-understanding/memorandum-of-understanding-on-economic-cooperation-and-trade-relations-between-indiana-and-the-united-kingdom')\n",
    "greatBritain.append('https://www.cnbc.com/2022/05/12/uk-economy-shrinks-in-march-grows-0point8percent-in-q1.html')\n",
    "greatBritain.append('https://www.ft.com/content/af0d30bd-f6ee-4ab4-abfe-94e58aa27b12')\n",
    "greatBritain.append('https://www.oecd.org/economy/united-kingdom-economic-snapshot/')\n",
    "greatBritain.append('https://eacea.ec.europa.eu/national-policies/eurydice/content/political-social-and-economic-background-and-trends-93_en')\n",
    "country.append(greatBritain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38de05f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordsDifference(first, second): # return words in first that are not in second\n",
    "    return [item for item in first if item not in second]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e78e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmpAlgorithm(url):\n",
    "    response = urllib.request.urlopen(url) # read article from the link\n",
    "    html = response.read().decode('UTF-8') # deocde the web content\n",
    "    text = stripTags(html).lower() # remove all non-alphanumeric words\n",
    "    fullwordlist = stripNonAlphaNum(text) # get word list\n",
    "    wordlist = removeStopwords(fullwordlist, getStopWords())  # remove stopwords\n",
    "    dictionary = wordListToFreqDict(wordlist) # get the frequency of each word\n",
    "    sorteddict = sortFreqDict(dictionary)\n",
    "\n",
    "    pos_word_list = [] # create list for positive words \n",
    "    count_pos_word = [] # create list for counts of positive words\n",
    "    neg_word_list = []  # create list for negative words \n",
    "    count_neg_word = [] # create list for counts of negative words\n",
    "\n",
    "    postive_words = get_positiveWords();\n",
    "    for i in postive_words:\n",
    "        count = KMPSearch(i, text) # search for positive words in the scrapped article\n",
    "        if count > 0:\n",
    "            pos_word_list.append(i)\n",
    "            count_pos_word.append(count)\n",
    "    \n",
    "    negative_words = get_negativeWords();\n",
    "    for i in negative_words:\n",
    "        count = KMPSearch(i, text) # search for negative words in the scrapped article\n",
    "        if count > 0:\n",
    "            neg_word_list.append(i)\n",
    "            count_neg_word.append(count)\n",
    "    \n",
    "    # get the neutral words by removing positive and negative words\n",
    "    neu_word_list = wordsDifference(wordlist, pos_word_list) \n",
    "    neu_word_list = wordsDifference(neu_word_list, neg_word_list)\n",
    "\n",
    "    totalPositiveWords = 0\n",
    "    for i in count_pos_word: # total number of positive words\n",
    "        totalPositiveWords += i\n",
    "\n",
    "    totalNegativeWords = 0\n",
    "    for i in count_neg_word: # total number of negative words\n",
    "        totalNegativeWords += i\n",
    "        \n",
    "    print(\"Total Positive Words : \" + str(totalPositiveWords))\n",
    "    print(\"Total Negative Words : \" + str(totalNegativeWords))\n",
    "    \n",
    "    count_neu_word = len(neu_word_list) # total number of neutral words\n",
    "    \n",
    "    result = [totalPositiveWords, totalNegativeWords, len(neu_word_list)]  # return number of all words\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3cd0eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rabinkarpAlgorithm(url):\n",
    "    response = urllib.request.urlopen(url) # read article from the link\n",
    "    html = response.read().decode('UTF-8') # deocde the web content\n",
    "    text = stripTags(html).lower() # remove all non-alphanumeric words\n",
    "    fullwordlist = stripNonAlphaNum(text) # get word list\n",
    "    wordlist = removeStopwords(fullwordlist, getStopWords()) # remove stopwords\n",
    "    dictionary = wordListToFreqDict(wordlist) # get the frequency of each word\n",
    "    sorteddict = sortFreqDict(dictionary)\n",
    "\n",
    "    pos_word_list = [] # create list for positive words \n",
    "    count_pos_word = [] # create list for counts of positive words\n",
    "    neg_word_list = []  # create list for negative words \n",
    "    count_neg_word = [] # create list for counts of negative words\n",
    "\n",
    "    postive_words = get_positiveWords();\n",
    "    for i in postive_words:\n",
    "        count = rabinkarp(text, i, 256, 101) # search for positive words in the scrapped article\n",
    "        if count > 0:\n",
    "            pos_word_list.append(i)\n",
    "            count_pos_word.append(count)\n",
    "        \n",
    "    negative_words = get_negativeWords();\n",
    "    for i in negative_words:\n",
    "        count = rabinkarp(text, i, 256, 101) # search for negative words in the scrapped article\n",
    "        if count > 0:\n",
    "            neg_word_list.append(i)\n",
    "            count_neg_word.append(count)\n",
    "\n",
    "    neu_word_list = wordsDifference(wordlist, pos_word_list)\n",
    "    neu_word_list = wordsDifference(neu_word_list, neg_word_list)\n",
    "    \n",
    "    totalPositiveWords = 0\n",
    "    for i in count_pos_word:  # total number of positive words\n",
    "        totalPositiveWords += i\n",
    "\n",
    "    totalNegativeWords = 0\n",
    "    for i in count_neg_word: # total number of negative words\n",
    "        totalNegativeWords += i\n",
    "        \n",
    "    print(\"Total Positive Words : \" + str(totalPositiveWords))\n",
    "    print(\"Total Negative Words : \" + str(totalNegativeWords))\n",
    "    \n",
    "    count_neu_word = len(neu_word_list) # total number of neutral words\n",
    "    \n",
    "    result = [totalPositiveWords, totalNegativeWords, len(neu_word_list)] # return number of all words\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75036459",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Result for Country 1 Article 1 :-\n",
      "Total Positive Words : 217\n",
      "Total Negative Words : 159\n",
      "Calculating Result for Country 1 Article 2 :-\n",
      "Total Positive Words : 180\n",
      "Total Negative Words : 170\n",
      "Calculating Result for Country 1 Article 3 :-\n",
      "Total Positive Words : 138\n",
      "Total Negative Words : 186\n",
      "Calculating Result for Country 1 Article 4 :-\n",
      "Total Positive Words : 72\n",
      "Total Negative Words : 52\n",
      "Calculating Result for Country 1 Article 5 :-\n",
      "Total Positive Words : 275\n",
      "Total Negative Words : 215\n",
      "\n",
      "Calculating Result for Country 2 Article 1 :-\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error EOF occurred in violation of protocol (_ssl.c:1129)>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSSLEOFError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:1346\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1345\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1346\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1347\u001b[0m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:1285\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1330\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:1280\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1280\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:1040\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1040\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1043\u001b[0m \n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:980\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 980\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:1454\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1452\u001b[0m     server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m-> 1454\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:500\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    496\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    497\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1040\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1040\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1309\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1309\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mSSLEOFError\u001b[0m: EOF occurred in violation of protocol (_ssl.c:1129)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(country[i])):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating Result for Country \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Article \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m :-\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mrabinkarpAlgorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# rabin karp algorithm\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# result = kmpAlgorithm(country[i][j]) # kmp algorithm\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# result = sentiment_analysis(country[i][j]) # using nltk library\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     countryNumberOfWords\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36mrabinkarpAlgorithm\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrabinkarpAlgorithm\u001b[39m(url):\n\u001b[1;32m----> 2\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# read article from the link\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     html \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# deocde the web content\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     text \u001b[38;5;241m=\u001b[39m stripTags(html)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;66;03m# remove all non-alphanumeric words\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:517\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    514\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    516\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 517\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    520\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:534\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    533\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 534\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m    535\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:1389\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py:1349\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[0;32m   1347\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m   1348\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[0;32m   1350\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m   1351\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error EOF occurred in violation of protocol (_ssl.c:1129)>"
     ]
    }
   ],
   "source": [
    "totalNumberOfWords = [] # total words\n",
    "countryAverageSentiment = [] # sentiment score for each country\n",
    "netSentiment = 0\n",
    "for i in range(len(country)):\n",
    "    countryNumberOfWords = []\n",
    "    for j in range(len(country[i])):\n",
    "        print(\"Calculating Result for Country \" + str(i+1) + \" Article \" + str(j+1) + \" :-\")\n",
    "        result = rabinkarpAlgorithm(country[i][j]) # rabin karp algorithm\n",
    "        # result = kmpAlgorithm(country[i][j]) # kmp algorithm\n",
    "        # result = sentiment_analysis(country[i][j]) # using nltk library\n",
    "        countryNumberOfWords.append(result)\n",
    "        res1 = \"{:.2f}\".format(result[0] / np.sum(result) * 100) # round to two decimal places\n",
    "        res2 = \"{:.2f}\".format(result[1] / np.sum(result) * 100) # round to two decimal places\n",
    "        netSentiment += np.round((float(res1) - float(res2)),2) # calculate net sentiment by subtrating postive with negative\n",
    "    totalNumberOfWords.append(countryNumberOfWords)\n",
    "    print()\n",
    "    countryAverageSentiment.append(np.round(netSentiment/5,2))\n",
    "\n",
    "#create country sentiment dictionary\n",
    "countrySentimentDict = {\"Japan\":countryAverageSentiment[0], \"China\":countryAverageSentiment[1], \"UAE\":countryAverageSentiment[2], \"Canada\":countryAverageSentiment[3], \"Great Britain\":countryAverageSentiment[4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d888ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w = 0.25 # bar width\n",
    "countryName = list(countrySentimentDict.keys())\n",
    "\n",
    "# plot bar chart for all articles categorized by country\n",
    "for i in range(len(totalNumberOfWords)):\n",
    "    positiveWords = [i[0] for i in totalNumberOfWords[i]]\n",
    "    negativeWords = [i[1] for i in totalNumberOfWords[i]]\n",
    "    neutralWords = [i[2] for i in totalNumberOfWords[i]]\n",
    "\n",
    "    bar1 = np.arange(len(totalNumberOfWords[0]))\n",
    "    bar2 = [x + w for x in bar1]\n",
    "    bar3 = [x + w for x in bar2]\n",
    "\n",
    "    plt.bar(bar1,positiveWords,w,label=\"Positive Words\")\n",
    "    plt.bar(bar2,negativeWords,w,label=\"Negative Words\")\n",
    "    plt.bar(bar3,neutralWords,w,label=\"Neutral Words\")\n",
    "\n",
    "    plt.xlabel(\"Articles\")\n",
    "    plt.ylabel(\"Word Count\")\n",
    "    plt.title(\"Sentiment Analysis of Articles of Country \" + countryName[i])\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d35e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Based on the analysis of net sentiment, the ranking of countries from best to worst is as below:- \\n\")\n",
    "sortedCountry = sortFreqDict(countrySentimentDict) # sort country sentiment dictionary\n",
    "sortedKey = list(sortedCountry.keys()) # sorted country names\n",
    "sortedValues = list(sortedCountry.values()) # sorted sentiment values\n",
    "\n",
    "for i in range(len(sortedCountry)):\n",
    "    print(str(i+1) + \"th Position : \" + sortedKey[i] + \" with average net sentiment of \" + str(sortedValues[i]) + \"\\n\")\n",
    "\n",
    "# plot bar chart to depict ranking\n",
    "plt.bar(sortedKey, sortedValues, fc=\"lightgray\", ec=\"black\")\n",
    "plt.xlabel(\"Country\")\n",
    "plt.ylabel(\"Average Net Sentiment\")\n",
    "plt.title(\"Rankings of Country\")\n",
    "\n",
    "for i in range(len(sortedKey)):\n",
    "    plt.text(i,sortedValues[i],sortedValues[i],ha=\"center\",va=\"bottom\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda80ff2",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83adb319",
   "metadata": {},
   "source": [
    "### * Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd # for dataframe\n",
    "import requests\n",
    "import folium # for map plotting\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c6428",
   "metadata": {},
   "source": [
    "### * Greedy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfffe23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedyAlgorithm(data, center, matrix_list):\n",
    "    unvisited = data.values.tolist() # list containing all locations\n",
    "    visited = [] # list containing visited locations\n",
    "    totalDistance = 0 # total distance travelled\n",
    "\n",
    "    visited = [x for x in unvisited if (center==x)] # insert center as first visited location\n",
    "    unvisited.remove(visited[0]) # remove center location\n",
    "\n",
    "    while (not len(unvisited) == 0): # run loop until all locations are traversed\n",
    "        shortestDistance = sys.maxsize # set maximum number to compare\n",
    "        currentNode = visited[-1] # set current node as last visited location\n",
    "        nextNode = [] # next node to visit\n",
    "        for i in range(len(unvisited)): # run loop for all locations in unvisited list\n",
    "            currentNextNode = unvisited[i] # current next node in unvisited list\n",
    "            distance = greedyCalculateDistance(data, matrix_list, currentNode, currentNextNode) # calculate the distance between the nodes\n",
    "            if ((distance < shortestDistance) and (not distance == 0)): # compare if the distance is shorter than previous\n",
    "                nextNode = currentNextNode # change the node to next node to visit\n",
    "                shortestDistance = distance # change the shortest distance\n",
    "        totalDistance += shortestDistance # append the final shortest distance after looping\n",
    "        unvisited.remove(nextNode) # remove the next node visited from unvisited list\n",
    "        visited.append(nextNode) # append the node in the visited list\n",
    "    # add the distance of last visited location and distribution center\n",
    "    totalDistance += greedyCalculateDistance(data, matrix_list, visited[-1],visited[0])\n",
    "    visited.append(visited[0]) # append the center as the last visited location\n",
    "    return (visited,totalDistance) # return the route with distance travelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f1b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedyCalculateDistance(data, matrix_list, origin, destination): # define function to calculate distance\n",
    "    locations = data.values.tolist() # convert the locations dataframe into list\n",
    "    startLoc = locations.index(origin) # get the index of origin location\n",
    "    finalLoc = locations.index(destination) # get the index of destination location\n",
    "\n",
    "    return matrix_list[startLoc][finalLoc] # return the distance between origin and destination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5845b79d",
   "metadata": {},
   "source": [
    "### * Naives Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94743fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naivesAlgorithm(data, center, distanceMatrix):\n",
    "    totalDistance = 0\n",
    "    route = []\n",
    "    stores = data.values.tolist()\n",
    "    stores_list = stores.copy()\n",
    "    stores_list.remove(center)\n",
    "    numbers = naiveGetIndexes(data, stores_list)\n",
    "    l = naivePermutation(numbers)\n",
    "    minDis = sys.maxsize\n",
    "    for i in l:\n",
    "        i.insert(0, naiveGetIndex(data, center))\n",
    "        i.append(naiveGetIndex(data, center))\n",
    "        totalDis = 0\n",
    "        for j in range(len(i) - 1):\n",
    "            totalDis += distanceMatrix[i[j]][i[j+1]]\n",
    "        if (minDis >= totalDis):\n",
    "            minDis = totalDis\n",
    "            route = i\n",
    "    totalDistance = minDis\n",
    "    storeOrderList = naiveGetLocations(stores,route)\n",
    "    return (storeOrderList,totalDistance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a3d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naivePermutation(lst):\n",
    "        if len(lst) == 0:\n",
    "            return []\n",
    "        if len(lst) == 1:\n",
    "            return [lst]\n",
    "        l = []\n",
    "        for i in range(len(lst)):\n",
    "            m = lst[i]\n",
    "            remLst = lst[:i] + lst[i + 1:]\n",
    "            for p in naivePermutation(remLst):\n",
    "                l.append([m] + p)\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6895c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveCalculateDistance(stores, matrix, origin, destination):\n",
    "        startLoc = stores.index(origin)\n",
    "        finalLoc = stores.index(destination)\n",
    "\n",
    "        return matrix[startLoc][finalLoc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4b5755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveGetIndexes(data, stores):\n",
    "        numbers = []\n",
    "        startingIndexofDataFrame = data.index[0]\n",
    "        for i in range(len(stores)):\n",
    "            index = (data[data['name']==stores[i][2]].index[0]) - startingIndexofDataFrame\n",
    "            numbers.append(index)\n",
    "        return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a3c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveGetIndex(data, location):\n",
    "        startingIndexofDataFrame = data.index[0]\n",
    "        index = (data[data['name'] == location[2]].index[0]) - startingIndexofDataFrame\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d8a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveGetLocations(stores, indexes):\n",
    "        storeRoute = []\n",
    "        for i in indexes:\n",
    "            storeRoute.append(stores[i])\n",
    "        return storeRoute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9654f275",
   "metadata": {},
   "source": [
    "### * Two Opt Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11411d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twoOpt_Algorithm(data, centre, distanceMatrix): # Two Opt Algorithm\n",
    "    store_list = data.values.tolist() # get stores list\n",
    "    best_route = [] # intialise best route\n",
    "    totalDistance = 0 # intialise total distance travelled\n",
    "    route = []\n",
    "    indexOfCentre = twoOpt_getIndex(data, centre) # get index of distribution centre\n",
    "    route.append(indexOfCentre) # append index of distribution centre\n",
    "    locationIndexes = twoOpt_getIndexes(data, store_list)\n",
    "    for x in range(0, len(locationIndexes)): # append index of other locations\n",
    "        if(x != indexOfCentre):\n",
    "            route.append(x)\n",
    "    route.append(indexOfCentre) # append index of distribution centre\n",
    "    best = route \n",
    "    improved = True # Perform Two-Opt Algorithm\n",
    "    while improved:\n",
    "        improved = False\n",
    "        for i in range(1, len(route) - 2):\n",
    "            for j in range(i + 1, len(route)):\n",
    "                if j - i == 1: continue\n",
    "                if twoOpt_cost_change(distanceMatrix, best[i - 1], best[i], best[j - 1], best[j]) < 0:  # check for cost change\n",
    "                    best[i:j] = best[j - 1:i - 1:-1]\n",
    "                    improved = True\n",
    "        route = best\n",
    "    best_route = twoOpt_getLocations(store_list, route) # change indices of locations to list\n",
    "    totalDistance = twoOpt_calculateDistance(distanceMatrix, route) # get total distance travelled\n",
    "    return (best_route,totalDistance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21249c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twoOpt_cost_change(distanceMatrix, n1, n2, n3, n4): # calculate cost change (cost refers to distance)\n",
    "        return distanceMatrix[n1][n3] + distanceMatrix[n2][n4] - distanceMatrix[n1][n2] - distanceMatrix[n3][n4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6193a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twoOpt_getIndex(data, location): # change location list into specific index\n",
    "    startingIndexofDataFrame = data.index[0]\n",
    "    index = (data[data['name'] == location[2]].index[0]) - startingIndexofDataFrame\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abdc392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twoOpt_getIndexes(data, route): # change all location into indices\n",
    "    numbers = []\n",
    "    startingIndexofDataFrame = data.index[0]\n",
    "    for i in range(len(route)):\n",
    "        index = (data[data['name']==route[i][2]].index[0]) - startingIndexofDataFrame\n",
    "        numbers.append(index)\n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f911f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twoOpt_getLocations(stores, indexes): # change indices back to locations list\n",
    "    storeRoute = []\n",
    "    for i in indexes:\n",
    "        storeRoute.append(stores[i])\n",
    "    return storeRoute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e7c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twoOpt_calculateDistance(matrixDistance, route): # calcualte the distance between location\n",
    "    totalRouteDistance = 0\n",
    "    for i in range(len(route)-1):\n",
    "        totalRouteDistance += matrixDistance[route[i]][route[i+1]]\n",
    "    return totalRouteDistance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0182866c",
   "metadata": {},
   "source": [
    "### * Calculating Routes and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787cf036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeRequest(startlat, startlong, endlat, endlong):\n",
    "    base_url = \"https://api.tomtom.com/routing/1/calculateRoute/\" # base url of TomTom API\n",
    "    api_key = \"7MkgGUp49plHiafrgGJCh83tt6bLwkDi\" # API key\n",
    "    url = \"%s/%s,%s:%s,%s/json?key=%s&traffic=false&routeType=shortest\" % (\n",
    "        base_url, str(startlat), str(startlong), str(endlat), str(endlong), api_key)\n",
    "    # logging.info(\"URL: %s\" % url)\n",
    "    result = requests.get(url) # request to API \n",
    "    return result.json() # return in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f874776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRoutePoints(data, direc, matrix):\n",
    "    print(\"-----Calculating Distances-----\")\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data)):\n",
    "            try:\n",
    "                # dir  = directions in json format\n",
    "                dir = makeRequest(data.iloc[i, 0], data.iloc[i, 1], data.iloc[j, 0], data.iloc[j, 1]) # request to TomTom API\n",
    "                direc[i][j] = dir['routes'][0]['legs'][0]['points'] # store points in 2d array\n",
    "                matrix[i][j] = int(dir['routes'][0]['summary']['lengthInMeters']) / 1000 # store distance between location in matrix\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            continue\n",
    "  \n",
    "    output = (matrix, direc)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce89b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBestCentre(matrix, stores_list): # find distribution centre\n",
    "    print(\"-----Picking Distribution Centre-----\")\n",
    "    bestdis = sys.maxsize # assign maximum number\n",
    "    best = -1 # assign a non-index\n",
    "    for i in range(len(matrix)):\n",
    "        totalDis = 0 # initialise total distace = 0\n",
    "        for j in range(len(matrix[i])):\n",
    "            totalDis += matrix[i][j] # calculate the total distance between all locations and location in index i\n",
    "        matrix[i].append(totalDis)\n",
    "        if (bestdis >= totalDis): # find the location with minimum total distance between all locations\n",
    "            bestdis = totalDis\n",
    "            best = i\n",
    "    bestCentre = stores_list[best] # get the index location from list of stores\n",
    "    return bestCentre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f248a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(data, bestCentre, matrix):\n",
    "    routeAndTotalDistance = greedyAlgorithm(data, bestCentre, matrix) # call greedy algorithm\n",
    "    route = routeAndTotalDistance[0]\n",
    "    totalDistance = routeAndTotalDistance[1]\n",
    "    routeToString = route[0][2]\n",
    "    for i in range(1, len(route)):\n",
    "        routeToString += \" ---> \" + route[i][2]\n",
    "    print(\"Greedy Algorithm Route: \" + routeToString) # print route\n",
    "    print(\"Total Distance Travelled : \" + str(totalDistance) + \" KM\") # print total distance\n",
    "    return routeAndTotalDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e1cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naives(data, bestCentre, matrix):\n",
    "    routeAndTotalDistance = naivesAlgorithm(data, bestCentre, matrix) # call naive algorithm\n",
    "    route = routeAndTotalDistance[0]\n",
    "    totalDistance = routeAndTotalDistance[1]\n",
    "    routeToString = route[0][2]\n",
    "    for i in range(1, len(route)):\n",
    "        routeToString += \" ---> \" + route[i][2]\n",
    "    print(\"Naives Algorithm Route: \" + routeToString) # print route\n",
    "    print(\"Total Distance Travelled : \" + str(totalDistance) + \" KM\") # print total distance\n",
    "    return routeAndTotalDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eae184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twoOpt(data, bestCentre, matrix):\n",
    "    routeAndTotalDistance = two_opt(data, bestCentre, matrix)\n",
    "    route = routeAndTotalDistance[0]\n",
    "    totalDistance = routeAndTotalDistance[1]\n",
    "    routeToString = route[0][2]\n",
    "    for i in range(1, len(route)):\n",
    "        routeToString += \" ---> \" + route[i][2]\n",
    "    print(\"Two Opt Algorithm Route: \" + routeToString)\n",
    "    print(\"Total Distance Travelled : \" + str(totalDistance) + \" KM\")\n",
    "    return routeAndTotalDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d2a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndexes(route, data): # convert route into index form of integers\n",
    "    numbers = []\n",
    "    startingIndexofDataFrame = data.index[0]\n",
    "    for i in range(len(route)):\n",
    "        index = (data[data['name'] == route[i][2]].index[0]) - startingIndexofDataFrame # index starts from 0\n",
    "        numbers.append(index)\n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateDistance(stores_list, origin, destination, matrix):\n",
    "    startLoc = stores_list.index(origin) # get index of origin\n",
    "    finalLoc = stores_list.index(destination) # get index of destination\n",
    "\n",
    "    return matrix[startLoc][finalLoc] # return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f186a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRouteAndDistance(stores_list, route, data, matrix):\n",
    "    distance = [] # create list for distance\n",
    "    x = 0\n",
    "    for i in range(len(route) - 1):\n",
    "        x += int(calculateDistance(stores_list, route[i], route[i + 1], matrix)) # calculate total distance of the route\n",
    "        distance.append(x)\n",
    "    routeAndDist = (getIndexes(route, data), distance) # return tuple of route and distance\n",
    "    return routeAndDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d7b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMap(route, stores_list, bestCentre, data, matrix, direc, country): # create map using TomTom API\n",
    "        routewithDistance = getRouteAndDistance(stores_list, route, data, matrix) # get route and total distance\n",
    "        map = folium.Map(location=[bestCentre[0], bestCentre[1]], zoom_start=8) # initialise map\n",
    "        for i in range(len(routewithDistance[0]) - 1):\n",
    "            j = direc[routewithDistance[0][i]][routewithDistance[0][i + 1]] # calculate distance between\n",
    "            if (i == len(routewithDistance[0]) - 2):\n",
    "                # plot markers for centre distribution\n",
    "                folium.Marker(location=[data.iloc[routewithDistance[0][i + 1], 0],\n",
    "                                        data.iloc[routewithDistance[0][i + 1], 1]],\n",
    "                              popup=(data.iloc[routewithDistance[0][i + 1], 2] + \" (Dc) \\nDis: \" +\n",
    "                                     str(routewithDistance[1][i]) + \"km\"),\n",
    "                              icon=folium.Icon(color='red', prefix='fa', icon='anchor')).add_to(map)\n",
    "            else:\n",
    "                # plot markers for other locations\n",
    "                num_Icon = folium.DivIcon(icon_size=(150, 36), icon_anchor=(4, 38),\n",
    "                                          html=\"\"\"<div style=\"font-size: 14pt; color : white\">{:d}</div>\"\"\".format(\n",
    "                                              i + 1))\n",
    "                folium.Marker(location=[data.iloc[routewithDistance[0][i + 1], 0],\n",
    "                                        data.iloc[routewithDistance[0][i + 1], 1]],\n",
    "                              popup=(data.iloc[routewithDistance[0][i + 1], 2] + \" \\nDis: \" +\n",
    "                                     str(routewithDistance[1][i]) + \"km\"),\n",
    "                              icon=folium.Icon(color='darkblue', icon='')).add_to(map)\n",
    "                folium.Marker(location=[data.iloc[routewithDistance[0][i + 1], 0],\n",
    "                                        data.iloc[routewithDistance[0][i + 1], 1]],\n",
    "                              popup=(data.iloc[routewithDistance[0][i + 1], 2] + \" \\nDis: \" +\n",
    "                                     str(routewithDistance[1][i]) + \"km\"), icon=num_Icon).add_to(map)\n",
    "            print(\"---ploting \" + str(data.iloc[routewithDistance[0][i], 2]) + \" to \" +\n",
    "                  str(data.iloc[routewithDistance[0][i + 1], 2]) + \"---\")\n",
    "            # plot route between locations\n",
    "            for k in range(len(j)):\n",
    "                if (k == 0):\n",
    "                    ori = (j[k]['latitude'], j[k]['longitude'])\n",
    "                    continue\n",
    "                des = (j[k]['latitude'], j[k]['longitude'])\n",
    "                folium.PolyLine(locations=[ori, des], line_opacity=0.5).add_to(map)\n",
    "                ori = (j[k]['latitude'], j[k]['longitude'])\n",
    "        print(\"~~~~~~~~~~~~END~~~~~~~~~~~\")\n",
    "        return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da0a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMap(map): # show map\n",
    "    display(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68775a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAll(data, country):\n",
    "    matrix = [[0 for i in range(0, len(data))] for j in range(0, len(data))] # matrix to store distance matrix\n",
    "    direc = [[dict() for i in range(len(data))] for j in range(len(data))] # dictionary to store route points\n",
    "    bestCentre = [] # distribution centre\n",
    "    totalDistance = 0 # total distance\n",
    "    stores_list = data.values.tolist() # get stores list\n",
    "\n",
    "    matrixAndDirec = getRoutePoints(data, direc, matrix) # \n",
    "    distancematrix = matrixAndDirec[0]\n",
    "    direc = matrixAndDirec[1]\n",
    "    bestCentre = findBestCentre(matrix, stores_list)\n",
    "#     routeAndTotalDistance = greedy(data, bestCentre, matrix)\n",
    "#     routeAndTotalDistance = naives(data, bestCentre, matrix)\n",
    "    routeAndTotalDistance = twoOpt(data, bestCentre, matrix)\n",
    "    routeTotalDistanceList.append(routeAndTotalDistance)\n",
    "    map = createMap(routeAndTotalDistance[0], stores_list, bestCentre, data, matrix, direc, country)\n",
    "    plotMap(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ddb94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"~~~~~~~~~~~~BEGIN~~~~~~~~~~~\")\n",
    "countries = [\"JP\",\"CN\",\"AE\",\"CA\",\"GB\"] #\"JP\",\"CN\",\"AE\",\"CA\",\"GB\"\n",
    "routeTotalDistanceList = [] # create list for route path and total distance\n",
    "sheet_id = '1-VMr9O2n1EEogU30DmAhRoF02xz0dmUaD-1143mSL-E' # google sheet unique ID\n",
    "df = pd.read_csv(f\"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv\") # read csv file\n",
    "\n",
    "for country in countries:\n",
    "    data = df.loc[df[\"state\"] == country, [\"latitude\", \"longitude\", \"name\"]].head(7) # get the locations details\n",
    "    data = data.reset_index(drop=True) # reset the index\n",
    "\n",
    "    runAll(data, country)\n",
    "\n",
    "countryDistance = {countries[i]: routeTotalDistanceList[i][1] for i in range(len(countries))} # dictionary for total distances\n",
    "sortedCountries = sorted(countryDistance.items(), key=lambda x: x[1]) # sort the dictionary based on distance values\n",
    "\n",
    "for i in range(len(sortedCountries)):\n",
    "    distance = \"{:.2f}\".format(sortedCountries[i][1])\n",
    "    print(\"Country in \" + str(i + 1) + \" th Position : \" + sortedCountries[i][0] + \" (\" + distance + \"KM)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c962e0",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb2424",
   "metadata": {},
   "source": [
    "### * Ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimumval(sentimentList, distanceList): # define function minimumval\n",
    "    minValues = [min(sentimentList),min(distanceList)] # return minimum values of sentiment and distance\n",
    "\n",
    "    return minValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5f053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximumval(sentimentList,distanceList): # define function maximumval\n",
    "    maxValues = [max(sentimentList),max(distanceList)] # return maximum values of sentiment and distance\n",
    "\n",
    "    return maxValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateSentimentScore(sentimentList,distanceList): # calculate sentiment score using min max normalization\n",
    "    # score = (value-min/max-min) x weight\n",
    "\n",
    "    sentimentScore = [] # initialise lsit for sentiment score\n",
    "    min = minimumval(sentimentList,distanceList)[0] # get the minimum sentiment score\n",
    "    max = maximumval(sentimentList,distanceList)[0] # get the maximum sentiment score\n",
    "    factor = max - min # denominator\n",
    "    for i in range(len(sentimentList)):\n",
    "        score = ((sentimentList[i] - (min-0.01))/factor)*50  # calculate sentiment score\n",
    "        sentimentScore.append(score)\n",
    "          \n",
    "    return sentimentScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b5b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateDistanceScore(sentimentList,distanceList): # calculate distance score using min max normalization\n",
    "    # score = (max-value/max-min) x weight\n",
    "\n",
    "    distanceScore = [] # initialise list for distance score\n",
    "    min = minimumval(sentimentList,distanceList)[1] # get the minimum distance score\n",
    "    max = maximumval(sentimentList,distanceList)[1] # get the maximum distance score\n",
    "    factor = max - min # denominator\n",
    "    for i in range(len(distanceList)):\n",
    "        score = (((max+0.01)-distanceList[i])/factor)*50  # calculate distance score\n",
    "        distanceScore.append(score)\n",
    "    \n",
    "    return distanceScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e21bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overallScore(sentimentList,distanceList): # calculate overall score based on sentiment and distance\n",
    "    #sentimentScore + distanceScore\n",
    "    overallScore = [] # initialise list for sentiment score\n",
    "    for i in range(len(sentimentList)):\n",
    "        overallScore.append(calculateSentimentScore(sentimentList,distanceList)[i] + calculateDistanceScore(sentimentList,distanceList)[i])\n",
    "    \n",
    "    return overallScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6182d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateProbability(sentimentList, distanceList): # calculate probability of selecting each country\n",
    "    prob = [] # intitialise list for probability\n",
    "    overall_Score = overallScore(sentimentList, distanceList) # calculate overall score\n",
    "    total = sum(overall_Score) # sum of all countries overall score\n",
    "    for i in range(len(overall_Score)):\n",
    "        temp = overall_Score[i] / total # calcuate probability using score per total score\n",
    "        prob.append(temp)\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de50414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortProb(sentimentList,distanceList): # sorting the country based on overall score\n",
    "    probability  = calculateProbability(sentimentList,distanceList) # caluclate probability of each country\n",
    "\n",
    "\n",
    "    # quicksort totalscore\n",
    "    def partition(l, r, list):\n",
    "        pivot, pointer = list[r], l # assign pivot and pointer\n",
    "        for i in range(l, r): \n",
    "            if list[i] <= pivot: # check if element is smaller than pivot\n",
    "                list[i], list[pointer] = list[pointer], list[i]\n",
    "                pointer += 1\n",
    "        list[pointer], list[r] = list[r], list[pointer]\n",
    "        return pointer\n",
    "\n",
    "    def quicksort(l, r, list):\n",
    "        if len(list) == 1:\n",
    "            return list\n",
    "        if l < r:\n",
    "            pi = partition(l, r, list)\n",
    "            quicksort(l, pi - 1, list)\n",
    "            quicksort(pi + 1, r, list)\n",
    "        return list\n",
    "\n",
    "\n",
    "    sortedProb = quicksort(0,len(probability)-1,probability) # call quick sort\n",
    "\n",
    "    sortedProb.reverse() # reverse the order after sorting\n",
    "\n",
    "    return sortedProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2430536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortCountries(countries,sentimentList,distanceList):\n",
    "    countrylist = countries # copy of countries\n",
    "    unsortedScore = calculateProbability(sentimentList, distanceList) # calculate probability\n",
    "    sortedOverallScore = sortProb(sentimentList,distanceList) # sort probability\n",
    "    sortedCountrylist = [] # final sorted countries\n",
    "    \n",
    "    # get rankings of country based on overall score\n",
    "    for i in range(len(sortedOverallScore)):\n",
    "        for j in range(len(unsortedScore)):\n",
    "            if(sortedOverallScore[i] == unsortedScore[j]):\n",
    "                sortedCountrylist.append(countries[j])\n",
    "\n",
    "    return sortedCountrylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPie(rankedCountries,scores): # plot pie chart\n",
    "    mylabels = [\"China\",\"UAE\",\"Great Britain\",\"Canada\",\"Japan\"] # set labels\n",
    "    y = [scores[0],scores[1],scores[2],scores[3],scores[4]] # set Y values\n",
    "    plt.pie(y, labels=mylabels, explode=[0.2, 0, 0, 0, 0], shadow=True, autopct=lambda p: '{:.4f}%'.format(p), startangle = 180)\n",
    "    plt.title(\"Probability of Selecting Country to Expand Business\") # set title\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c974827e",
   "metadata": {},
   "source": [
    "### Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85c2ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ranking(countries, sentimentList, distanceList): # ranking the countries\n",
    "    rankedCountries = sortCountries(countries, sentimentList, distanceList) # sort countries\n",
    "\n",
    "    scores = sortProb(sentimentList, distanceList) # sort scores\n",
    "\n",
    "    print(\"\\n\\t\\tThe ranking on the countries based on the score \")\n",
    "    print(\"\\n-------------------------------------------------------------------------\")\n",
    "\n",
    "    for i in range(len(rankedCountries)): # print the rankings of countries\n",
    "        format_score = \"{:.5f}\".format(scores[i])\n",
    "        print(str(i + 1) + \"st position : \" + rankedCountries[i] + \" with the probability of \" + format_score)\n",
    "\n",
    "    print(\"The most recomended country is \" +rankedCountries[0])\n",
    "    print(\"\\n-------------------------------------------------------------------------\")\n",
    "    \n",
    "    plotPie(rankedCountries,scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb291c2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "distanceList = list(countryDistance.values())\n",
    "Ranking(countries, countryAverageSentiment, distanceList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
